{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' import modules '''\n",
    "import time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from logger import Logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Define an easy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=320, bias=True)\n",
       "  (fc2): Linear(in_features=320, out_features=160, bias=True)\n",
       "  (fc3): Linear(in_features=160, out_features=80, bias=True)\n",
       "  (fc4): Linear(in_features=80, out_features=40, bias=True)\n",
       "  (fc5): Linear(in_features=40, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully connected neural network with three hidden layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 320)\n",
    "        self.fc2 = nn.Linear(320, 160)\n",
    "        self.fc3 = nn.Linear(160, 80)\n",
    "        self.fc4 = nn.Linear(80, 40)\n",
    "        self.fc5 = nn.Linear(40, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "\n",
    "model = Net() \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "Hyper-parameters and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hyper-parameters'''\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Prapare data'''\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))]\n",
    "                   )),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))]\n",
    "                   )),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "Define train and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train and test'''\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # print(data.shape) [64,1,28,28]\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    # loss function already averages over batch size\n",
    "    test_loss /= len(test_loader)\n",
    "    acccuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, '\n",
    "          'Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss,\n",
    "                                               correct,\n",
    "                                               len(test_loader.dataset),\n",
    "                                               acccuracy))\n",
    "    return test_loss, acccuracy, data\n",
    "\n",
    "def test_train(epoch):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    # loss function already averages over batch size\n",
    "    train_loss /= len(train_loader)\n",
    "    acccuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, '\n",
    "          'Accuracy: {}/{} ({:.0f}%)\\n'.format(train_loss,\n",
    "                                               correct,\n",
    "                                               len(train_loader.dataset),\n",
    "                                               acccuracy))\n",
    "    return train_loss, acccuracy, data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "Train and record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.335186\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.276842\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.211098\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.136191\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.363432\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.844934\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.537819\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.529223\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.449302\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.498455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:29: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3786, Accuracy: 8868/10000 (88%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:51: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.3850, Accuracy: 53152/60000 (88%)\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.01s \n",
      "-----------------------------------------------------------------------------------------\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.413723\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.192235\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.365040\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.222258\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.400789\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.367563\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.256143\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.358769\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.268255\n"
     ]
    }
   ],
   "source": [
    "''' train'''\n",
    "best_loss = None\n",
    "logger_train = Logger('./logs/Momentum/0.5_trian')\n",
    "logger_val = Logger('./logs/Momentum/0.5_val')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(epoch)\n",
    "    test_loss, test_acc, images = test(epoch)\n",
    "    train_loss, train_acc, _ = test_train(epoch)\n",
    "    print('-' * 89)\n",
    "    timeLength = time.time() - epoch_start_time\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s '.format(\n",
    "        epoch, time.time() - epoch_start_time))\n",
    "    print('-' * 89)\n",
    "    best_loss = test_loss\n",
    "    with open('model.pt', 'wb') as fp:\n",
    "            state = model.state_dict()\n",
    "            torch.save(state, fp)\n",
    "    # ================================================================== #\n",
    "    #                        Tensorboard Logging                         #\n",
    "    # ================================================================== #\n",
    "    # 1. Log scalar values (scalar summary)\n",
    "    info = { 'loss': test_loss, 'accuracy': test_acc }\n",
    "    for tag, value in info.items():\n",
    "        logger_val.scalar_summary(tag, value, epoch+1)\n",
    "    info = { 'loss': train_loss, 'accuracy': train_acc }\n",
    "    for tag, value in info.items():\n",
    "        logger_train.scalar_summary(tag, value, epoch+1)\n",
    "\n",
    "    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "    for tag, value in model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger_train.histo_summary(tag, value.data.cpu().numpy(), epoch+1)\n",
    "            logger_train.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch+1)\n",
    "\n",
    "    # 3. Log training images (image summary)\n",
    "    info = { 'images': images.view(-1, 28, 28)[:10].cpu().numpy() }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "            logger_val.image_summary(tag, images, epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
